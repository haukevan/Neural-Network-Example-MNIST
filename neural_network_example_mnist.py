# -*- coding: utf-8 -*-
"""Neural Network Example MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14W-F_ooBnjZWhhb3LahA_GO9UQ3yYpRz
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import math

import csv
from google.colab import files
import io

#choose file from computer
#choose train csv data - WILL TAKE TIME TO UPLOAD
file = files.upload()

df = pd.read_csv('mnist_train.csv')
df.head()
#show data head

data = np.array(df)
m, n = data.shape
print(m, n)
np.random.shuffle(data)

#---Test train split----

#take first 1000 data for test data
#transpose data first so each example is a column '.T'
data_test = data[0:1000].T
y_test = data_test[0]
X_test = data_test[1:n]

#train data will be 1000 to 60,000
data_train = data[1000:m].T
y_train = data_train[0]
X_train = data_train[1:n]
#_,m_train = X_train.shape

y_train

def init_params():
  W1 = np.random.randn(10, 784) - 0.5
  b1 = np.random.randn(10, 1) - 0.5
  W2 = np.random.randn(10, 10) - 0.5
  b2 = np.random.randn(10, 1) - 0.5
  return W1, b1, W2, b2

#returns max of 0 to Z - if Z <=0, return 0, else return Z
def ReLU(Z):
  return np.maximum(Z, 0)

#softmax activation function - returns probability of sum of 
# each column divided by the element we want
def softmax(Z):
  A = np.exp(Z) / sum(np.exp(Z))
  return A

def forward_prop(W1, b1, W2, b2, X):
  Z1 = W1.dot(X) + b1
  A1 = ReLU(Z1)
  Z2 = W2.dot(A1) + b2
  A2 = softmax(Z2)
  return Z1, A1, Z2, A2

def deriv_ReLU(Z):
  return Z > 0

def one_hot(y):
  #create correctly sized array 
  one_hot_y = np.zeros((y.size, y.max() + 1))
  one_hot_y[np.arange(y.size), y] = 1
  one_hot_y = one_hot_y.T
  return one_hot_y

def back_prop(Z1, A1, Z2, A2, W2, X, y):
  m = y.size
  one_hot_y = one_hot(y)
  dZ2 = A2 - one_hot_y
  dW2 = 1 / m * dZ2.dot(A1.T)
  db2 = 1 / m * np.sum(dZ2)
  dZ1 = W2.T.dot(dZ2) * deriv_ReLU(Z1)
  dW1 = 1 / m * dZ1.dot(X.T)
  db1 = 1 / m * np.sum(dZ1)
  return dW1, db1, dW2, db2

def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):
  W1 = W1 - alpha * dW1
  b1 = b1 - alpha * db1
  W2 = W2 - alpha * dW2
  b2 = b2 - alpha * db2
  return W1, b1, W2, b2

def get_predictions(A2):
  return np.argmax(A2, 0)

def get_accuracy(predictions, y):
  print(predictions, y)
  return np.sum(predictions == y) / y.size

def gradient_decent(X, y, alpha, iterations):
  W1, b1, W2, b2 = init_params()

  for i in range(iterations):
    Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)
    dW1, db1, dW2, db2 = back_prop(Z1, A1, Z2, A2, W2, X, y)
    W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)
    #print every 10 iterations
    if (i % 10 == 0):
      print("Iteration Number: ", i)
      predictions = get_predictions(A2)
      print("Accuracy: ", get_accuracy(predictions, y))

  return W1, b1, W2, b2

X_train

W1, b1, W2, b2 = gradient_decent(X_train, y_train, 0.10, 100)